{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Process\n",
    "\n",
    "# from functions import read_sort_save, replace_content, process_text, split, wordcounts_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'or', 'nine', 'here', 'more', 'we', 'ever', \"'s\", 'enough', 'when', 'behind', 'five', 'however', 'whereafter', 'another', 'none', 'cannot', 'move', 'anywhere', 'only', 'everything', 'for', 'without', 'well', 'anyway', 'very', 'anything', 'ourselves', 'forty', 'with', \"'re\", 'hers', 'some', '‘d', 'why', 'most', 'own', 'formerly', 'can', 'moreover', 'name', \"'ve\", 'down', 'part', 'throughout', 'using', 'either', 'is', 'onto', 'whole', 'never', 'somehow', 'give', 'so', 'mine', 'call', 'neither', 'thru', 'therefore', '’ll', 'us', 'many', 'see', 'hereby', 'ours', 'an', 'anyone', 'its', 'our', 'bottom', 'into', '’s', 'even', 'whatever', 'she', 'itself', 'serious', 'say', 'keep', 'was', 'any', 'almost', 'per', 'seeming', 'a', 'among', 'go', 'ten', \"'d\", 'who', 'already', 'since', 'thereupon', \"'m\", 'doing', 'front', 'themselves', 'quite', 'put', 'will', 'latter', 'whereupon', 'this', 'noone', 'elsewhere', 'seems', 'until', 'eight', 'it', 'above', 'along', '‘re', 'side', 'while', 'wherever', 'via', 'mostly', 'hereafter', 'i', 'all', 'otherwise', 'across', 'made', 'perhaps', 'nor', 'besides', 'there', 'anyhow', 'am', 'twenty', 'whose', 'hence', 'few', 'their', 'toward', \"'ll\", 'latterly', 'because', \"n't\", 'every', '‘ll', 'beyond', 'against', 'sometime', 'show', 'then', 'must', 'rather', 'at', 'me', 'though', 'which', 'next', 'now', 'eleven', 'ca', 'empty', 'both', 'off', 'did', '’d', 'something', 'the', 'himself', 'does', 'had', 'he', 'just', '‘s', 'same', 'others', 'to', 'due', 'one', 'thereafter', 'herself', 'always', 'become', 'twelve', 'where', 'various', 'afterwards', 'someone', 'yourself', 'former', 'by', 'her', 'what', 'still', 'whoever', 'your', 'would', 'under', 'sixty', 'became', 'first', 'do', '’m', 'n‘t', 'becoming', 'yours', 'whereas', 'often', 'everyone', 'amongst', 'beforehand', 'whither', 'together', 'of', 'but', 'nowhere', '‘m', 'sometimes', 'each', 'yet', 'four', 'that', 'towards', 'herein', 'namely', 'been', 'fifteen', 'they', 'full', 'too', 'should', 'whether', 'except', 'his', 'unless', 'although', 'becomes', 'alone', 'therein', 'up', 'within', 'make', 'no', 'used', 'regarding', 'fifty', 'two', 'be', 'seem', 'myself', 'again', 'are', 'upon', 'whenever', 'six', 'further', 'thence', 'nevertheless', 'during', 'please', 'could', 'n’t', 'out', 'as', '’ve', 'other', 'through', 'meanwhile', 'beside', 'last', 'really', 'before', 'might', 'may', 'over', 'these', 'take', 'amount', 'third', 'below', 'least', 'whereby', 'than', 'somewhere', 'else', 'about', 'indeed', 'how', 'hundred', 'hereupon', 'such', 'him', 'get', 'once', 'has', 'whence', 'you', 'whom', 'back', 'nobody', 'not', '‘ve', '’re', 'were', 're', 'if', 'on', 'between', 'around', 'wherein', 'and', 'in', 'seemed', 'top', 'also', 'them', 'thus', 'those', 'being', 'have', 'much', 'several', 'less', 'three', 'thereby', 'everywhere', 'after', 'from', 'yourselves', 'done', 'my', 'nothing'}\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "redict = {\n",
    "\"are n't\" : \"aren't\",   \"ca n't\" : \"can't\",     \"could n't\" : \"couldn't\",   \"did n't\" : \"didn't\",       \"does n't\" : \"doesn't\",\\\n",
    "\"do n't\" : \"don't\",     \"had n't\" : \"hadn't\",   \"has n't\" : \"hasn't\",       \"have n't\" : \"haven't\",     \"h e'd\" : \"he'd\",\\\n",
    "\"h e'll\" : \"he'll\",     \"h e's\" : \"he's\",       \"i 'd\" : \"i'd\",             \"i 'll\" : \"i'll\",           \"i 'm\" : \"i'm\",\\\n",
    "\"i 've\" : \"i've\",       \"is n't\" : \"isn't\",     \"i t's\" : \"it's\",           \"le t's\" : \"let's\",         \"must n't\" : \"mustn't\",\\\n",
    "\"sha n't\" : \"shan't\",   \"sh e'd\" : \"she'd\",     \"sh e'll\" : \"she'll\",       \"sh e's\" : \"she's\",         \"should n't\" : \"shouldn't\",\\\n",
    "\"tha t's\" : \"that's\",   \"ther e's\" : \"there's\", \"the y'll\" : \"they'll\",     \"the y're\" : \"they're\",     \"the y've\" : \"they've\",\\\n",
    "\"w e'd\" : \"we'd\",       \"w e're\" : \"we're\",     \"w e've\" : \"we've\",         \"were n't\" : \"weren't\",     \"wha t'll\" : \"what'll\",\\\n",
    "\"wha t're\" : \"what're\", \"wha t's\" : \"what's\",   \"wha t've\" : \"what've\",     \"wher e's\" : \"where's\",     \"wh o'd\" : \"who'd\",\\\n",
    "\"wh o'll\" : \"who'll\",   \"wh o're\" : \"who're\",   \"wh o's\" : \"who's\",         \"wh o've\" : \"who've\",       \"wo n't\" : \"won't\",\\\n",
    "\"would n't\" : \"wouldn't\",\"yo u'd\" : \"you'd\",    \"yo u'll\" : \"you'll\",       \"yo u're\" : \"you're\",       \"yo u've\" : \"you've\",\\\n",
    "# \" 's\" : \"'s\",         # \" 're\": \"'re\",    \n",
    "\"new zealand\" : \"\",     \"<p>\" : \"\",             \"<h>\" : \"\",                 \" @ \" : \"\",                 \"@\" : \"\",\n",
    "\"\\n\" : \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of items to remove from word count dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list= [\n",
    "    \"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "    \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "    \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "    \"ms\", \"dr\", \"mrs\", \".\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function based on dict, replaces key with the value on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_content(dict_replace, target):\n",
    "    for check, replacer in list(dict_replace.items()):\n",
    "        target = target.replace(check, replacer)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to read .csv file as pandas dataframe, sort values by year, and save as .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sort_save(filename1, filename2):\n",
    "    dataframe = pd.read_csv(filename1, usecols=['year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "    dataframe.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "    dataframe.to_pickle(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create Word Count Dictionary which filters out stopwords and named entitites\n",
    "\n",
    "## First, parse through article and lowercase all words\n",
    "\n",
    "## Next, make corrections to words with apostrophers using redict dictionary\n",
    "\n",
    "## Append words from processed document to list with stopwords (wordfreq1) or list without stopwords (wordfreq2)\n",
    "\n",
    "## Return counter dictionaries of each list, save each list (stopwords/nostopwords) as .pkl files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ AND UNDERSTAND HOW FUNCTION WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(iteration, n):  \n",
    "   \n",
    "   quotient, remainder = divmod(len(iteration), n)\n",
    "\n",
    "   split_data = [\n",
    "      \n",
    "      iteration[\n",
    "         # FLOOR\n",
    "         i * quotient + min(i, remainder)\n",
    "         :\n",
    "         # CEILING\n",
    "         (i + 1) * quotient + min(i + 1, remainder)\n",
    "\n",
    "         ] for i in range(6)\n",
    "   ]\n",
    "\n",
    "   split_data_dictionary = {x[0] : x[1] for x in enumerate(split_data)}\n",
    "   \n",
    "   return split_data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, L1, L2):\n",
    "    text_data = text.lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "    for ent in document:\n",
    "        if ent.ent_type:\n",
    "            L1.append(ent.text)\n",
    "        else:\n",
    "            L2.append(ent.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcounts(file1_input, file1_output, file2_output):\n",
    "    \n",
    "    L1 = []\n",
    "    L2 = []  \n",
    "\n",
    "    dataframe = pd.read_pickle(file1_input)\n",
    "\n",
    "    sub_list = split(dataframe['article_text_Ngram_stopword_lemmatize'], 6)\n",
    "\n",
    "    for i in sub_list:\n",
    "        for j in sub_list[i]:\n",
    "            process_text(j, L1, L2)    \n",
    "\n",
    "    count_stopwords = Counter(word for word in L1)\n",
    "    count_nostopwords = Counter(word for word in L1)\n",
    "\n",
    "    with open(file1_output, 'wb') as handle:\n",
    "        pickle.dump(count_stopwords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(file2_output, 'wb') as handle:\n",
    "        pickle.dump(count_nostopwords, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After checking word count lists, clean for named entities and nlp terms from data using removedict and stopword lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcounts_clean(file1_input, file2_input, remove_list, stopword_list, file1_output, file2_output):\n",
    "\n",
    "    stopwords_dataframe = pd.read_pickle(file1_input)\n",
    "    nostopwords_dataframe = pd.read_pickle(file2_input)\n",
    "\n",
    "    count_stopwords = dict(zip(stopwords_dataframe[0], stopwords_dataframe[1]))\n",
    "\n",
    "    count_nostopwords = dict(zip(nostopwords_dataframe[0], nostopwords_dataframe[1]))\n",
    "\n",
    "    for i in remove_list:\n",
    "        if i in count_stopwords:\n",
    "            del count_stopwords[i]\n",
    "        if i in count_nostopwords:\n",
    "            del count_nostopwords[i]\n",
    "\n",
    "    for i in stopword_list:\n",
    "        if i in count_stopwords:\n",
    "            del count_stopwords[i]\n",
    "        if i in count_nostopwords:\n",
    "            del count_nostopwords[i]\n",
    "\n",
    "    df_count_stopwords = pd.DataFrame(Counter(count_stopwords).most_common(500))\n",
    "    df_count_nostopwords = pd.DataFrame(Counter(count_nostopwords).most_common(500))\n",
    "\n",
    "    with open(file1_output, 'wb') as handle:\n",
    "        pickle.dump(df_count_stopwords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(file2_output, 'wb') as handle:\n",
    "        pickle.dump(df_count_nostopwords, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcounts_print_sample(dictionary):\n",
    "    for i in enumerate(Counter(dictionary).most_common(300)):\n",
    "        if i[0] < 20 or i[0] > 279:\n",
    "            print(i)\n",
    "        elif i[0] == (149):\n",
    "            print(\"\\n\\t ... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open and read excel files as pandas dataframes: cols = ['year', 'article_text_Ngram_stopword_lemmatize']\n",
    "## Sort articles by year\n",
    "## Save as .pkl file\n",
    "\n",
    "## Note: Pakistan (PK) and South Africa (ZA) were omitted due to insufficient articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Australia AU\n",
    "read_sort_save('AU_domestic_Ngram_stopword_lematize.csv', 'df_au.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_au.pkl', 'df_counts_au1.pkl', 'df_counts_au2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_items = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \"'\", \"''\", \"'m\", \"/\", \"'ll\", \"*\", \"'d\", \"'ve\", \"m\"]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_au1.pkl', \n",
    "               'df_counts_au2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_au1.pkl',\n",
    "               'df_counts_au2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('time', 99918))\n",
      "(1, ('people', 83357))\n",
      "(2, ('like', 73990))\n",
      "(3, ('new', 69815))\n",
      "(4, ('work', 65096))\n",
      "(5, ('use', 63113))\n",
      "(6, ('come', 57492))\n",
      "(7, ('need', 51243))\n",
      "(8, ('government', 51129))\n",
      "(9, ('look', 49322))\n",
      "(10, ('think', 47609))\n",
      "(11, ('know', 46152))\n",
      "(12, ('way', 45194))\n",
      "(13, ('include', 45178))\n",
      "(14, ('game', 43195))\n",
      "(15, ('good', 41740))\n",
      "(16, ('want', 41397))\n",
      "(17, ('high', 39172))\n",
      "(18, ('market', 38863))\n",
      "(19, ('world', 38654))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('condition', 10379))\n",
      "(281, ('appear', 10360))\n",
      "(282, ('league', 10360))\n",
      "(283, ('understand', 10271))\n",
      "(284, ('personal', 10251))\n",
      "(285, ('campaign', 10240))\n",
      "(286, ('benefit', 10196))\n",
      "(287, ('target', 10172))\n",
      "(288, ('labor', 10115))\n",
      "(289, ('goal', 10113))\n",
      "(290, ('hope', 10105))\n",
      "(291, ('history', 10080))\n",
      "(292, ('clear', 10078))\n",
      "(293, ('gold', 10043))\n",
      "(294, ('statement', 10021))\n",
      "(295, ('note', 10012))\n",
      "(296, ('land', 10003))\n",
      "(297, ('film', 9998))\n",
      "(298, ('example', 9953))\n",
      "(299, ('bring', 9951))\n"
     ]
    }
   ],
   "source": [
    "df_counts_au_nostopwords = pd.read_pickle('df_counts_au2.pkl')\n",
    "\n",
    "df_counts_au_nostopwords = dict(zip(df_counts_au_nostopwords[0], df_counts_au_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_au_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bangladesh BD\n",
    "read_sort_save('BD_domestic_Ngram_stopword_lematize.csv', 'df_bd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_bd.pkl', 'df_counts_bd1.pkl', 'df_counts_bd2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = ['bangladeshi', 'sheikh', 'crore', \\\n",
    "                'tk', 'bnp', 'bangabandhu', 'bangladesh', \\\n",
    "               'rahman', 'hossain', 'hasina', 'upazila', \\\n",
    "               'bangabandhu', 'bangla', 'indian', 'myanmar', \\\n",
    "               'chittagong', 'khan', 'rohingya', 'chowdhury', 'prof'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_bd1.pkl', \n",
    "               'df_counts_bd2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_bd1.pkl',\n",
    "               'df_counts_bd2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('country', 22510))\n",
      "(1, ('government', 20966))\n",
      "(2, ('people', 19966))\n",
      "(3, ('minister', 12601))\n",
      "(4, ('time', 12261))\n",
      "(5, ('work', 11527))\n",
      "(6, ('police', 10164))\n",
      "(7, ('come', 9810))\n",
      "(8, ('high', 9434))\n",
      "(9, ('use', 8819))\n",
      "(10, ('new', 8614))\n",
      "(11, ('include', 8222))\n",
      "(12, ('case', 8117))\n",
      "(13, ('add', 8097))\n",
      "(14, ('area', 7970))\n",
      "(15, ('world', 7732))\n",
      "(16, ('need', 7700))\n",
      "(17, ('bank', 7421))\n",
      "(18, ('official', 7324))\n",
      "(19, ('report', 7164))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('initiative', 2090))\n",
      "(281, ('control', 2089))\n",
      "(282, ('award', 2085))\n",
      "(283, ('coronavirus', 2084))\n",
      "(284, ('tax', 2083))\n",
      "(285, ('financial', 2075))\n",
      "(286, ('raise', 2074))\n",
      "(287, ('management', 2070))\n",
      "(288, ('quality', 2067))\n",
      "(289, ('effort', 2067))\n",
      "(290, ('conduct', 2065))\n",
      "(291, ('thing', 2057))\n",
      "(292, ('recently', 2045))\n",
      "(293, ('join', 2043))\n",
      "(294, ('grow', 2041))\n",
      "(295, ('conference', 2034))\n",
      "(296, ('medical', 2033))\n",
      "(297, ('address', 2033))\n",
      "(298, ('field', 2032))\n",
      "(299, ('complete', 2029))\n"
     ]
    }
   ],
   "source": [
    "df_counts_bd_nostopwords = pd.read_pickle('df_counts_bd2.pkl')\n",
    "\n",
    "df_counts_bd_nostopwords = dict(zip(df_counts_bd_nostopwords[0], df_counts_bd_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_bd_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Canada CA\n",
    "read_sort_save('CA_domestic_Ngram_stopword_lematize.csv', 'df_ca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_ca.pkl', 'df_counts_ca1.pkl', 'df_counts_ca2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = ['ottowa', 'ontario', 'cbc', 'toolong', 'vancouver' ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_ca1.pkl', \n",
    "               'df_counts_ca2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_ca1.pkl',\n",
    "               'df_counts_ca2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('time', 101831))\n",
      "(1, ('people', 96695))\n",
      "(2, ('use', 81684))\n",
      "(3, ('new', 80842))\n",
      "(4, ('like', 78735))\n",
      "(5, ('work', 78459))\n",
      "(6, ('come', 69959))\n",
      "(7, ('company', 62410))\n",
      "(8, ('include', 60010))\n",
      "(9, ('game', 57024))\n",
      "(10, ('look', 55700))\n",
      "(11, ('know', 54572))\n",
      "(12, ('want', 54483))\n",
      "(13, ('right', 52872))\n",
      "(14, ('information', 52481))\n",
      "(15, ('comment', 52001))\n",
      "(16, ('need', 51774))\n",
      "(17, ('way', 50782))\n",
      "(18, ('city', 50557))\n",
      "(19, ('team', 49435))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('general', 12725))\n",
      "(281, ('serve', 12710))\n",
      "(282, ('involve', 12703))\n",
      "(283, ('shot', 12681))\n",
      "(284, ('reason', 12643))\n",
      "(285, ('control', 12616))\n",
      "(286, ('related', 12598))\n",
      "(287, ('course', 12530))\n",
      "(288, ('land', 12522))\n",
      "(289, ('bank', 12466))\n",
      "(290, ('strong', 12449))\n",
      "(291, ('management', 12399))\n",
      "(292, ('data', 12377))\n",
      "(293, ('energy', 12353))\n",
      "(294, ('access', 12342))\n",
      "(295, ('list', 12297))\n",
      "(296, ('growth', 12263))\n",
      "(297, ('encourage', 12262))\n",
      "(298, ('film', 12255))\n",
      "(299, ('role', 12249))\n"
     ]
    }
   ],
   "source": [
    "df_counts_ca_nostopwords = pd.read_pickle('df_counts_ca2.pkl')\n",
    "\n",
    "df_counts_ca_nostopwords = dict(zip(df_counts_ca_nostopwords[0], df_counts_ca_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_ca_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## United Kingdom GB\n",
    "read_sort_save('GB_domestic_Ngram_stopword_lematize.csv', 'df_gb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_gb.pkl', 'df_counts_gb1.pkl', 'df_counts_gb2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = ['uk', 'trump']\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_gb1.pkl', \n",
    "               'df_counts_gb2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_gb1.pkl',\n",
    "               'df_counts_gb2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('people', 48863))\n",
      "(1, ('time', 48544))\n",
      "(2, ('like', 39139))\n",
      "(3, ('work', 38795))\n",
      "(4, ('new', 37046))\n",
      "(5, ('come', 33054))\n",
      "(6, ('use', 32345))\n",
      "(7, ('know', 25840))\n",
      "(8, ('look', 24954))\n",
      "(9, ('world', 24617))\n",
      "(10, ('way', 24316))\n",
      "(11, ('want', 24149))\n",
      "(12, ('need', 23568))\n",
      "(13, ('include', 22977))\n",
      "(14, ('think', 21758))\n",
      "(15, ('life', 21306))\n",
      "(16, ('game', 20243))\n",
      "(17, ('good', 19542))\n",
      "(18, ('government', 18847))\n",
      "(19, ('right', 18820))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('word', 5778))\n",
      "(281, ('space', 5772))\n",
      "(282, ('reason', 5762))\n",
      "(283, ('style', 5703))\n",
      "(284, ('possible', 5695))\n",
      "(285, ('sure', 5693))\n",
      "(286, ('fight', 5693))\n",
      "(287, ('likely', 5691))\n",
      "(288, ('black', 5685))\n",
      "(289, ('room', 5679))\n",
      "(290, ('student', 5664))\n",
      "(291, ('chief', 5658))\n",
      "(292, ('council', 5655))\n",
      "(293, ('carry', 5629))\n",
      "(294, ('fund', 5613))\n",
      "(295, ('available', 5593))\n",
      "(296, ('fire', 5589))\n",
      "(297, ('performance', 5582))\n",
      "(298, ('light', 5581))\n",
      "(299, ('major', 5579))\n"
     ]
    }
   ],
   "source": [
    "df_counts_gb_nostopwords = pd.read_pickle('df_counts_gb2.pkl')\n",
    "\n",
    "df_counts_gb_nostopwords = dict(zip(df_counts_gb_nostopwords[0], df_counts_gb_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_gb_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ghana GH\n",
    "read_sort_save('GH_domestic_Ngram_stopword_lematize.csv', 'df_gh.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_gh.pkl', 'df_counts_gh1.pkl', 'df_counts_gh2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = ['ghana', 'accra', 'ghanaian', 'ghanaians', 'npp', 'ndc', 'addo', 'nana',\\\n",
    "               'mahama', 'akufo', 'gh', 'kumasi', 'fm', 'prof'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_gh1.pkl', \n",
    "               'df_counts_gh2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_gh1.pkl',\n",
    "               'df_counts_gh2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('country', 28303))\n",
      "(1, ('government', 26010))\n",
      "(2, ('people', 22788))\n",
      "(3, ('president', 21453))\n",
      "(4, ('time', 16691))\n",
      "(5, ('come', 14847))\n",
      "(6, ('new', 14518))\n",
      "(7, ('work', 14492))\n",
      "(8, ('use', 13881))\n",
      "(9, ('national', 13612))\n",
      "(10, ('service', 13458))\n",
      "(11, ('state', 13292))\n",
      "(12, ('need', 13132))\n",
      "(13, ('party', 13013))\n",
      "(14, ('company', 12639))\n",
      "(15, ('development', 12539))\n",
      "(16, ('school', 12535))\n",
      "(17, ('public', 12129))\n",
      "(18, ('know', 11194))\n",
      "(19, ('region', 11045))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('death', 3135))\n",
      "(281, ('large', 3121))\n",
      "(282, ('building', 3118))\n",
      "(283, ('vote', 3110))\n",
      "(284, ('indicate', 3082))\n",
      "(285, ('seek', 3078))\n",
      "(286, ('association', 3077))\n",
      "(287, ('individual', 3073))\n",
      "(288, ('live', 3067))\n",
      "(289, ('demand', 3062))\n",
      "(290, ('peace', 3040))\n",
      "(291, ('vice', 3038))\n",
      "(292, ('establish', 3019))\n",
      "(293, ('close', 3016))\n",
      "(294, ('left', 3014))\n",
      "(295, ('central', 3013))\n",
      "(296, ('tell', 3010))\n",
      "(297, ('candidate', 2988))\n",
      "(298, ('farmer', 2983))\n",
      "(299, ('senior', 2983))\n"
     ]
    }
   ],
   "source": [
    "df_counts_gh_nostopwords = pd.read_pickle('df_counts_gh2.pkl')\n",
    "\n",
    "df_counts_gh_nostopwords = dict(zip(df_counts_gh_nostopwords[0], df_counts_gh_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_gh_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Hong Kong appears to have much less data than other countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hong Kong HK\n",
    "read_sort_save('HK_domestic_Ngram_stopword_lematize.csv', 'df_hk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_hk.pkl', 'df_counts_hk1.pkl', 'df_counts_hk2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = ['lt', 'gt', '/p', 'p', 'hk', 'class=', 'http', 'asia', 'lam', 'co', \\\n",
    "               'chan', 'p1', 'beijing', 'href=', 'wp'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_hk1.pkl', \n",
    "               'df_counts_hk2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_hk1.pkl',\n",
    "               'df_counts_hk2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('amp', 3972))\n",
      "(1, ('company', 3826))\n",
      "(2, ('government', 3738))\n",
      "(3, ('people', 3349))\n",
      "(4, ('business', 3084))\n",
      "(5, ('time', 3051))\n",
      "(6, ('market', 2900))\n",
      "(7, ('new', 2787))\n",
      "(8, ('law', 2549))\n",
      "(9, ('report', 2392))\n",
      "(10, ('include', 2237))\n",
      "(11, ('work', 2053))\n",
      "(12, ('public', 1988))\n",
      "(13, ('city', 1940))\n",
      "(14, ('high', 1933))\n",
      "(15, ('use', 1928))\n",
      "(16, ('country', 1920))\n",
      "(17, ('bank', 1790))\n",
      "(18, ('group', 1746))\n",
      "(19, ('come', 1723))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('able', 447))\n",
      "(281, ('process', 447))\n",
      "(282, ('form', 446))\n",
      "(283, ('cause', 445))\n",
      "(284, ('view', 444))\n",
      "(285, ('seek', 443))\n",
      "(286, ('role', 443))\n",
      "(287, ('reason', 441))\n",
      "(288, ('domestic', 441))\n",
      "(289, ('remain', 441))\n",
      "(290, ('range', 441))\n",
      "(291, ('activity', 440))\n",
      "(292, ('study', 439))\n",
      "(293, ('charge', 439))\n",
      "(294, ('sign', 439))\n",
      "(295, ('despite', 438))\n",
      "(296, ('body', 438))\n",
      "(297, ('likely', 437))\n",
      "(298, ('current', 436))\n",
      "(299, ('general', 436))\n"
     ]
    }
   ],
   "source": [
    "df_counts_hk_nostopwords = pd.read_pickle('df_counts_hk2.pkl')\n",
    "\n",
    "df_counts_hk_nostopwords = dict(zip(df_counts_hk_nostopwords[0], df_counts_hk_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_hk_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ireland IE\n",
    "read_sort_save('IE_domestic_Ngram_stopword_lematize.csv', 'df_ie.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_ie.pkl', 'df_counts_ie1.pkl', 'df_counts_ie2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'ireland', 'irish', 'dublin', 'cooky', 'limerick', 'co', 'galway', 'uk', 'derry', 'yea'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_ie1.pkl', \n",
    "               'df_counts_ie2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_ie1.pkl',\n",
    "               'df_counts_ie2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('time', 77019))\n",
      "(1, ('use', 72634))\n",
      "(2, ('people', 61005))\n",
      "(3, ('come', 50201))\n",
      "(4, ('work', 48931))\n",
      "(5, ('new', 47556))\n",
      "(6, ('like', 42155))\n",
      "(7, ('website', 37221))\n",
      "(8, ('team', 32182))\n",
      "(9, ('home', 32092))\n",
      "(10, ('site', 31766))\n",
      "(11, ('look', 31373))\n",
      "(12, ('game', 29067))\n",
      "(13, ('know', 29061))\n",
      "(14, ('need', 28496))\n",
      "(15, ('day', 28163))\n",
      "(16, ('good', 28144))\n",
      "(17, ('place', 28050))\n",
      "(18, ('include', 27576))\n",
      "(19, ('family', 27170))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('miss', 7790))\n",
      "(281, ('range', 7776))\n",
      "(282, ('option', 7753))\n",
      "(283, ('drive', 7744))\n",
      "(284, ('later', 7725))\n",
      "(285, ('finish', 7722))\n",
      "(286, ('film', 7719))\n",
      "(287, ('hope', 7701))\n",
      "(288, ('remember', 7687))\n",
      "(289, ('raise', 7632))\n",
      "(290, ('click', 7628))\n",
      "(291, ('agree', 7623))\n",
      "(292, ('control', 7617))\n",
      "(293, ('grow', 7615))\n",
      "(294, ('question', 7609))\n",
      "(295, ('action', 7604))\n",
      "(296, ('saw', 7593))\n",
      "(297, ('claim', 7591))\n",
      "(298, ('force', 7572))\n",
      "(299, ('act', 7561))\n"
     ]
    }
   ],
   "source": [
    "df_counts_ie_nostopwords = pd.read_pickle('df_counts_ie2.pkl')\n",
    "\n",
    "df_counts_ie_nostopwords = dict(zip(df_counts_ie_nostopwords[0], df_counts_ie_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_ie_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## India IN\n",
    "read_sort_save('IN_domestic_Ngram_stopword_lematize.csv', 'df_in.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_in.pkl', 'df_counts_in1.pkl', 'df_counts_in2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "               'rs', 'delhi', 'crore', 'ist', 'singh', 'gmt', 'bjp', 'modi', 'indi', 'lakh', \\\n",
    "               'pradesh', 'facebook'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_in1.pkl', \n",
    "               'df_counts_in2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_in1.pkl',\n",
    "               'df_counts_in2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('government', 73660))\n",
      "(1, ('time', 69275))\n",
      "(2, ('state', 65463))\n",
      "(3, ('new', 61042))\n",
      "(4, ('people', 59907))\n",
      "(5, ('come', 57492))\n",
      "(6, ('like', 56214))\n",
      "(7, ('work', 49861))\n",
      "(8, ('company', 49197))\n",
      "(9, ('country', 45834))\n",
      "(10, ('use', 43461))\n",
      "(11, ('high', 43275))\n",
      "(12, ('minister', 41688))\n",
      "(13, ('police', 37447))\n",
      "(14, ('need', 37376))\n",
      "(15, ('case', 37257))\n",
      "(16, ('film', 36489))\n",
      "(17, ('team', 34827))\n",
      "(18, ('market', 34272))\n",
      "(19, ('issue', 34170))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('near', 9795))\n",
      "(281, ('old', 9742))\n",
      "(282, ('consider', 9723))\n",
      "(283, ('committee', 9719))\n",
      "(284, ('foreign', 9700))\n",
      "(285, ('earlier', 9641))\n",
      "(286, ('accuse', 9582))\n",
      "(287, ('device', 9580))\n",
      "(288, ('let', 9561))\n",
      "(289, ('quality', 9546))\n",
      "(290, ('course', 9522))\n",
      "(291, ('happen', 9478))\n",
      "(292, ('seek', 9474))\n",
      "(293, ('scheme', 9469))\n",
      "(294, ('carry', 9464))\n",
      "(295, ('stand', 9431))\n",
      "(296, ('current', 9427))\n",
      "(297, ('conduct', 9406))\n",
      "(298, ('customer', 9387))\n",
      "(299, ('station', 9375))\n"
     ]
    }
   ],
   "source": [
    "df_counts_in_nostopwords = pd.read_pickle('df_counts_in2.pkl')\n",
    "\n",
    "df_counts_in_nostopwords = dict(zip(df_counts_in_nostopwords[0], df_counts_in_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_in_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jamaica JM\n",
    "read_sort_save('JM_domestic_Ngram_stopword_lematize.csv', 'df_jm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_jm.pkl', 'df_counts_jm1.pkl', 'df_counts_jm2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'caribbean', 'jamaica', 'jamaican', 'st', 'jamaicans', 'kingston', 'montego', \\\n",
    "                'reggae', 'pnp'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_jm1.pkl', \n",
    "               'df_counts_jm2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_jm1.pkl',\n",
    "               'df_counts_jm2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('time', 29117))\n",
      "(1, ('minister', 29005))\n",
      "(2, ('work', 27897))\n",
      "(3, ('people', 27789))\n",
      "(4, ('government', 26488))\n",
      "(5, ('country', 25510))\n",
      "(6, ('school', 25263))\n",
      "(7, ('new', 23885))\n",
      "(8, ('come', 22996))\n",
      "(9, ('child', 21634))\n",
      "(10, ('use', 21393))\n",
      "(11, ('need', 21383))\n",
      "(12, ('high', 20116))\n",
      "(13, ('include', 19472))\n",
      "(14, ('person', 19439))\n",
      "(15, ('community', 18486))\n",
      "(16, ('public', 18438))\n",
      "(17, ('development', 18362))\n",
      "(18, ('national', 18258))\n",
      "(19, ('business', 18248))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('develop', 4744))\n",
      "(281, ('final', 4718))\n",
      "(282, ('write', 4715))\n",
      "(283, ('bring', 4711))\n",
      "(284, ('black', 4687))\n",
      "(285, ('reduce', 4679))\n",
      "(286, ('manager', 4671))\n",
      "(287, ('comment', 4654))\n",
      "(288, ('foreign', 4652))\n",
      "(289, ('lose', 4623))\n",
      "(290, ('initiative', 4623))\n",
      "(291, ('risk', 4618))\n",
      "(292, ('amp', 4603))\n",
      "(293, ('environment', 4596))\n",
      "(294, ('citizen', 4585))\n",
      "(295, ('encourage', 4559))\n",
      "(296, ('away', 4540))\n",
      "(297, ('partner', 4538))\n",
      "(298, ('covid', 4521))\n",
      "(299, ('competition', 4500))\n"
     ]
    }
   ],
   "source": [
    "df_counts_jm_nostopwords = pd.read_pickle('df_counts_jm2.pkl')\n",
    "\n",
    "df_counts_jm_nostopwords = dict(zip(df_counts_jm_nostopwords[0], df_counts_jm_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_jm_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kenya KE\n",
    "read_sort_save('KE_domestic_Ngram_stopword_lematize.csv', 'df_ke.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_ke.pkl', 'df_counts_ke1.pkl', 'df_counts_ke2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'kenya', 'nairobi', 'raila', 'uhuru', 'mp', 'mombasa', 'odinga', 'kenyan', 'ruto', \\\n",
    "                'kenyatta' \n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_ke1.pkl', \n",
    "               'df_counts_ke2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_ke1.pkl',\n",
    "               'df_counts_ke2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('government', 28224))\n",
      "(1, ('country', 25395))\n",
      "(2, ('people', 22643))\n",
      "(3, ('time', 21596))\n",
      "(4, ('president', 21091))\n",
      "(5, ('county', 20705))\n",
      "(6, ('come', 17559))\n",
      "(7, ('work', 16792))\n",
      "(8, ('new', 16562))\n",
      "(9, ('use', 16317))\n",
      "(10, ('police', 15603))\n",
      "(11, ('national', 15497))\n",
      "(12, ('public', 14465))\n",
      "(13, ('court', 13989))\n",
      "(14, ('high', 13657))\n",
      "(15, ('like', 13510))\n",
      "(16, ('service', 13347))\n",
      "(17, ('need', 13321))\n",
      "(18, ('include', 12640))\n",
      "(19, ('school', 12497))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('rule', 3616))\n",
      "(281, ('question', 3602))\n",
      "(282, ('game', 3588))\n",
      "(283, ('operation', 3581))\n",
      "(284, ('final', 3578))\n",
      "(285, ('travel', 3552))\n",
      "(286, ('clear', 3545))\n",
      "(287, ('action', 3537))\n",
      "(288, ('vehicle', 3536))\n",
      "(289, ('station', 3517))\n",
      "(290, ('effort', 3510))\n",
      "(291, ('far', 3486))\n",
      "(292, ('award', 3472))\n",
      "(293, ('produce', 3470))\n",
      "(294, ('tax', 3467))\n",
      "(295, ('target', 3462))\n",
      "(296, ('building', 3459))\n",
      "(297, ('programme', 3453))\n",
      "(298, ('economy', 3448))\n",
      "(299, ('period', 3441))\n"
     ]
    }
   ],
   "source": [
    "df_counts_ke_nostopwords = pd.read_pickle('df_counts_ke2.pkl')\n",
    "\n",
    "df_counts_ke_nostopwords = dict(zip(df_counts_ke_nostopwords[0], df_counts_ke_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_ke_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sri Lanka LK\n",
    "read_sort_save('LK_domestic_Ngram_stopword_lematize.csv', 'df_lk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_lk.pkl', 'df_counts_lk1.pkl', 'df_counts_lk2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'sri', 'lanka', 'colombo', 'lankan', 'tamil', 'rs', 'rajapaksa', 'sirisena', \\\n",
    "                'sinhala', 'buddhist', 'tamils', 'mahinda', 'ceylon'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_lk1.pkl', \n",
    "               'df_counts_lk2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_lk1.pkl',\n",
    "               'df_counts_lk2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('country', 18584))\n",
      "(1, ('government', 17051))\n",
      "(2, ('people', 13226))\n",
      "(3, ('president', 12330))\n",
      "(4, ('minister', 11345))\n",
      "(5, ('time', 10794))\n",
      "(6, ('state', 9263))\n",
      "(7, ('new', 9108))\n",
      "(8, ('come', 7976))\n",
      "(9, ('use', 7914))\n",
      "(10, ('work', 7629))\n",
      "(11, ('high', 7518))\n",
      "(12, ('world', 7488))\n",
      "(13, ('need', 7023))\n",
      "(14, ('include', 6911))\n",
      "(15, ('international', 6898))\n",
      "(16, ('national', 6603))\n",
      "(17, ('development', 6586))\n",
      "(18, ('issue', 6155))\n",
      "(19, ('political', 6095))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('institution', 1789))\n",
      "(281, ('recent', 1786))\n",
      "(282, ('launch', 1782))\n",
      "(283, ('represent', 1781))\n",
      "(284, ('history', 1780))\n",
      "(285, ('facility', 1780))\n",
      "(286, ('investigation', 1770))\n",
      "(287, ('vehicle', 1761))\n",
      "(288, ('living', 1761))\n",
      "(289, ('recently', 1757))\n",
      "(290, ('request', 1756))\n",
      "(291, ('produce', 1753))\n",
      "(292, ('training', 1751))\n",
      "(293, ('away', 1751))\n",
      "(294, ('left', 1750))\n",
      "(295, ('remain', 1748))\n",
      "(296, ('story', 1745))\n",
      "(297, ('appoint', 1744))\n",
      "(298, ('travel', 1744))\n",
      "(299, ('website', 1742))\n"
     ]
    }
   ],
   "source": [
    "df_counts_lk_nostopwords = pd.read_pickle('df_counts_lk2.pkl')\n",
    "\n",
    "df_counts_lk_nostopwords = dict(zip(df_counts_lk_nostopwords[0], df_counts_lk_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_lk_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Malaysia MY\n",
    "read_sort_save('MY_domestic_Ngram_stopword_lematize.csv', 'df_my.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_my.pkl', 'df_counts_my1.pkl', 'df_counts_my2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'malaysia', 'bhd', 'malaysians', 'sabah', 'lumpur', 'umno', 'kuala', 'datuk', \\\n",
    "                'najib', 'bn', 'malay', 'pas', 'mohd'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_my1.pkl', \n",
    "               'df_counts_my2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_my1.pkl',\n",
    "               'df_counts_my2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('government', 34692))\n",
      "(1, ('country', 25729))\n",
      "(2, ('people', 25653))\n",
      "(3, ('new', 24631))\n",
      "(4, ('time', 24115))\n",
      "(5, ('use', 23905))\n",
      "(6, ('minister', 21194))\n",
      "(7, ('state', 20642))\n",
      "(8, ('company', 18220))\n",
      "(9, ('high', 18003))\n",
      "(10, ('work', 17900))\n",
      "(11, ('like', 17897))\n",
      "(12, ('come', 17464))\n",
      "(13, ('need', 17413))\n",
      "(14, ('comment', 17125))\n",
      "(15, ('right', 15180))\n",
      "(16, ('public', 15155))\n",
      "(17, ('group', 15099))\n",
      "(18, ('report', 15070))\n",
      "(19, ('include', 14931))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('accord', 4053))\n",
      "(281, ('stop', 4050))\n",
      "(282, ('instead', 4049))\n",
      "(283, ('begin', 4045))\n",
      "(284, ('revenue', 4040))\n",
      "(285, ('capital', 4038))\n",
      "(286, ('deal', 4020))\n",
      "(287, ('special', 4006))\n",
      "(288, ('past', 3961))\n",
      "(289, ('reduce', 3946))\n",
      "(290, ('office', 3927))\n",
      "(291, ('chairman', 3927))\n",
      "(292, ('learn', 3927))\n",
      "(293, ('profit', 3923))\n",
      "(294, ('performance', 3918))\n",
      "(295, ('opposition', 3917))\n",
      "(296, ('customer', 3896))\n",
      "(297, ('understand', 3877))\n",
      "(298, ('body', 3877))\n",
      "(299, ('require', 3875))\n"
     ]
    }
   ],
   "source": [
    "df_counts_my_nostopwords = pd.read_pickle('df_counts_my2.pkl')\n",
    "\n",
    "df_counts_my_nostopwords = dict(zip(df_counts_my_nostopwords[0], df_counts_my_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_my_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nigeria NG\n",
    "read_sort_save('NG_domestic_Ngram_stopword_lematize.csv', 'df_ng.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_ng.pkl', 'df_counts_ng1.pkl', 'df_counts_ng2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'nigeria', 'nigerians', 'lagos', 'abuja', 'pdp', 'apc', 'nigerian', 'buhari', \\\n",
    "                'boko', 'niger'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_ng1.pkl', \n",
    "               'df_counts_ng2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_ng1.pkl',\n",
    "               'df_counts_ng2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('state', 166160))\n",
      "(1, ('government', 101682))\n",
      "(2, ('people', 87846))\n",
      "(3, ('country', 77670))\n",
      "(4, ('president', 63295))\n",
      "(5, ('time', 53674))\n",
      "(6, ('come', 50587))\n",
      "(7, ('governor', 44600))\n",
      "(8, ('national', 41904))\n",
      "(9, ('know', 41285))\n",
      "(10, ('like', 40458))\n",
      "(11, ('federal', 39649))\n",
      "(12, ('new', 39288))\n",
      "(13, ('need', 39026))\n",
      "(14, ('work', 36905))\n",
      "(15, ('use', 36815))\n",
      "(16, ('party', 34917))\n",
      "(17, ('election', 33964))\n",
      "(18, ('member', 33079))\n",
      "(19, ('issue', 32753))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('away', 9264))\n",
      "(281, ('access', 9255))\n",
      "(282, ('special', 9247))\n",
      "(283, ('rule', 9247))\n",
      "(284, ('past', 9245))\n",
      "(285, ('water', 9180))\n",
      "(286, ('food', 9119))\n",
      "(287, ('appeal', 9099))\n",
      "(288, ('worker', 9090))\n",
      "(289, ('platform', 9066))\n",
      "(290, ('secretary', 9060))\n",
      "(291, ('campaign', 9023))\n",
      "(292, ('represent', 9013))\n",
      "(293, ('conduct', 8995))\n",
      "(294, ('view', 8989))\n",
      "(295, ('require', 8945))\n",
      "(296, ('demand', 8898))\n",
      "(297, ('available', 8894))\n",
      "(298, ('resource', 8888))\n",
      "(299, ('quality', 8821))\n"
     ]
    }
   ],
   "source": [
    "df_counts_ng_nostopwords = pd.read_pickle('df_counts_ng2.pkl')\n",
    "\n",
    "df_counts_ng_nostopwords = dict(zip(df_counts_ng_nostopwords[0], df_counts_ng_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_ng_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Zealand NZ\n",
    "read_sort_save('NZ_domestic_Ngram_stopword_lematize.csv', 'df_nz.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_nz.pkl', 'df_counts_nz1.pkl', 'df_counts_nz2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'auckland', 'gt', 'nz', 'zealand', 'christchurch', 'maori', 'kiwi', 'zealanders', \\\n",
    "                'otago', 'te', ' ', 'ers', 'dunedin'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_nz1.pkl', \n",
    "               'df_counts_nz2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_nz1.pkl',\n",
    "               'df_counts_nz2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('people', 75023))\n",
      "(1, ('time', 66007))\n",
      "(2, ('work', 62730))\n",
      "(3, ('new', 56268))\n",
      "(4, ('come', 45495))\n",
      "(5, ('like', 44970))\n",
      "(6, ('need', 43598))\n",
      "(7, ('use', 40259))\n",
      "(8, ('government', 36245))\n",
      "(9, ('include', 34029))\n",
      "(10, ('look', 33152))\n",
      "(11, ('business', 33040))\n",
      "(12, ('want', 32814))\n",
      "(13, ('company', 32397))\n",
      "(14, ('high', 32369))\n",
      "(15, ('way', 31763))\n",
      "(16, ('world', 31423))\n",
      "(17, ('good', 31215))\n",
      "(18, ('know', 30530))\n",
      "(19, ('change', 29393))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('clear', 7988))\n",
      "(281, ('idea', 7960))\n",
      "(282, ('improve', 7909))\n",
      "(283, ('finish', 7888))\n",
      "(284, ('learn', 7877))\n",
      "(285, ('prime', 7823))\n",
      "(286, ('sure', 7817))\n",
      "(287, ('worker', 7816))\n",
      "(288, ('covid', 7816))\n",
      "(289, ('claim', 7813))\n",
      "(290, ('organisation', 7810))\n",
      "(291, ('significant', 7804))\n",
      "(292, ('later', 7799))\n",
      "(293, ('management', 7796))\n",
      "(294, ('save', 7788))\n",
      "(295, ('announce', 7748))\n",
      "(296, ('district', 7740))\n",
      "(297, ('environment', 7738))\n",
      "(298, ('supply', 7731))\n",
      "(299, ('age', 7730))\n"
     ]
    }
   ],
   "source": [
    "df_counts_nz_nostopwords = pd.read_pickle('df_counts_nz2.pkl')\n",
    "\n",
    "df_counts_nz_nostopwords = dict(zip(df_counts_nz_nostopwords[0], df_counts_nz_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_nz_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Philippines PH\n",
    "read_sort_save('PH_domestic_Ngram_stopword_lematize.csv', 'df_ph.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_ph.pkl', 'df_counts_ph1.pkl', 'df_counts_ph2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'f', 'fr', 'n', 'r', 'wh', 'are', 'ne', 'or', 'al', 'ut', 'manila', 'ver', \\\n",
    "                'philippine', 'filipino', 'pint', 'duterte', 'de', 'g', 'curt', 'd', 'filipinos', \\\n",
    "                'ff', 'barangay', 'll', 't', 'me', 's', 'sunstar', 'webster', 'a'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    " \n",
    "# a = \"cmputr\"           # incorrect spelling\n",
    "# print(\"original text: \"+str(a))\n",
    "\n",
    "counts_ph2_fix = {}\n",
    "\n",
    "for i, j in counts_ph2_copy.items():\n",
    "    b = TextBlob(str(i))\n",
    "    counts_ph2_fix[str(b.correct())] = j\n",
    "    \n",
    "# b = TextBlob(a)\n",
    " \n",
    "# prints the corrected spelling\n",
    "# print(\"corrected text: \"+str(b.correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_ph1.pkl', \n",
    "               'df_counts_ph2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_ph1.pkl',\n",
    "               'df_counts_ph2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('frm', 158563))\n",
      "(1, ('nt', 126112))\n",
      "(2, ('mre', 62080))\n",
      "(3, ('city', 59707))\n",
      "(4, ('ur', 53152))\n",
      "(5, ('gvernment', 52619))\n",
      "(6, ('wuld', 51676))\n",
      "(7, ('time', 50058))\n",
      "(8, ('ther', 48107))\n",
      "(9, ('yu', 44998))\n",
      "(10, ('abut', 44134))\n",
      "(11, ('peple', 43767))\n",
      "(12, ('president', 42575))\n",
      "(13, ('tw', 40438))\n",
      "(14, ('new', 39723))\n",
      "(15, ('use', 38808))\n",
      "(16, ('like', 36488))\n",
      "(17, ('cuntry', 36090))\n",
      "(18, ('nly', 34588))\n",
      "(19, ('nw', 33001))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('men', 7738))\n",
      "(281, ('challenge', 7729))\n",
      "(282, ('address', 7720))\n",
      "(283, ('test', 7679))\n",
      "(284, ('live', 7677))\n",
      "(285, ('study', 7654))\n",
      "(286, ('administratin', 7646))\n",
      "(287, ('arund', 7611))\n",
      "(288, ('facility', 7609))\n",
      "(289, ('peratins', 7594))\n",
      "(290, ('pay', 7570))\n",
      "(291, ('suth', 7529))\n",
      "(292, ('design', 7514))\n",
      "(293, ('far', 7460))\n",
      "(294, ('miss', 7457))\n",
      "(295, ('let', 7457))\n",
      "(296, ('plant', 7412))\n",
      "(297, ('arrest', 7401))\n",
      "(298, ('reflect', 7385))\n",
      "(299, ('smething', 7375))\n"
     ]
    }
   ],
   "source": [
    "df_counts_ph_nostopwords = pd.read_pickle('df_counts_ph2.pkl')\n",
    "\n",
    "df_counts_ph_nostopwords = dict(zip(df_counts_ph_nostopwords[0], df_counts_ph_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_ph_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIX SPELLING ERRORS IN DICTIONARY WITH TEXTBLOB function, REPLACE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('from', 158563))\n",
      "(1, ('are', 62080))\n",
      "(2, ('city', 59707))\n",
      "(3, ('or', 53152))\n",
      "(4, ('government', 52619))\n",
      "(5, ('would', 51676))\n",
      "(6, ('time', 50058))\n",
      "(7, ('you', 44998))\n",
      "(8, ('but', 44134))\n",
      "(9, ('people', 43767))\n",
      "(10, ('president', 42575))\n",
      "(11, ('new', 39723))\n",
      "(12, ('use', 38808))\n",
      "(13, ('like', 36488))\n",
      "(14, ('country', 36090))\n",
      "(15, ('only', 34588))\n",
      "(16, ('national', 31992))\n",
      "(17, ('right', 29393))\n",
      "(18, ('through', 27853))\n",
      "(19, ('high', 27290))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('meeting', 7349))\n",
      "(281, ('budget', 7328))\n",
      "(282, ('air', 7301))\n",
      "(283, ('fact', 7288))\n",
      "(284, ('island', 7276))\n",
      "(285, ('vehicle', 7242))\n",
      "(286, ('performance', 7238))\n",
      "(287, ('training', 7233))\n",
      "(288, ('class', 7213))\n",
      "(289, ('coming', 7191))\n",
      "(290, ('webster', 7183))\n",
      "(291, ('known', 7138))\n",
      "(292, ('ensure', 7106))\n",
      "(293, ('real', 7066))\n",
      "(294, ('available', 7037))\n",
      "(295, ('agreement', 7017))\n",
      "(296, ('various', 7008))\n",
      "(297, ('ban', 7008))\n",
      "(298, ('fire', 7008))\n",
      "(299, ('early', 7001))\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# a = \"cmputr\"           # incorrect spelling\n",
    "# print(\"original text: \"+str(a))\n",
    "# b = TextBlob(a)\n",
    " \n",
    "# prints the corrected spelling\n",
    "# print(\"corrected text: \"+str(b.correct()))\n",
    "\n",
    "df_counts_ph_nostopwords_fix = {}\n",
    "\n",
    "for i, j in df_counts_ph_nostopwords.items():\n",
    "    b = TextBlob(str(i))\n",
    "    df_counts_ph_nostopwords_fix[str(b.correct())] = j\n",
    "\n",
    "wordcounts_print_sample(df_counts_ph_nostopwords_fix)\n",
    "\n",
    "with open('df_counts_ph2.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_counts_ph_nostopwords_fix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pakistan PK\n",
    "## Omitted due to insufficient articles from NOW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Singapore SG\n",
    "read_sort_save('SG_domestic_Ngram_stopword_lematize.csv', 'df_sg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_sg.pkl', 'df_counts_sg1.pkl', 'df_counts_sg2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_items = [\n",
    "                's', 'singapore', 'singaporeans', 'asia', 'facebook'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_sg1.pkl', \n",
    "               'df_counts_sg2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_sg1.pkl',\n",
    "               'df_counts_sg2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('from', 158563))\n",
      "(1, ('are', 62080))\n",
      "(2, ('city', 59707))\n",
      "(3, ('or', 53152))\n",
      "(4, ('government', 52619))\n",
      "(5, ('would', 51676))\n",
      "(6, ('time', 50058))\n",
      "(7, ('you', 44998))\n",
      "(8, ('but', 44134))\n",
      "(9, ('people', 43767))\n",
      "(10, ('president', 42575))\n",
      "(11, ('new', 39723))\n",
      "(12, ('use', 38808))\n",
      "(13, ('like', 36488))\n",
      "(14, ('country', 36090))\n",
      "(15, ('only', 34588))\n",
      "(16, ('national', 31992))\n",
      "(17, ('right', 29393))\n",
      "(18, ('through', 27853))\n",
      "(19, ('high', 27290))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('meeting', 7349))\n",
      "(281, ('budget', 7328))\n",
      "(282, ('air', 7301))\n",
      "(283, ('fact', 7288))\n",
      "(284, ('island', 7276))\n",
      "(285, ('vehicle', 7242))\n",
      "(286, ('performance', 7238))\n",
      "(287, ('training', 7233))\n",
      "(288, ('class', 7213))\n",
      "(289, ('coming', 7191))\n",
      "(290, ('webster', 7183))\n",
      "(291, ('known', 7138))\n",
      "(292, ('ensure', 7106))\n",
      "(293, ('real', 7066))\n",
      "(294, ('available', 7037))\n",
      "(295, ('agreement', 7017))\n",
      "(296, ('various', 7008))\n",
      "(297, ('ban', 7008))\n",
      "(298, ('fire', 7008))\n",
      "(299, ('early', 7001))\n"
     ]
    }
   ],
   "source": [
    "df_counts_sg_nostopwords = pd.read_pickle('df_counts_sg2.pkl')\n",
    "\n",
    "df_counts_sg_nostopwords = dict(zip(df_counts_sg_nostopwords[0], df_counts_sg_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_ph_nostopwords_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tanzania TZ\n",
    "read_sort_save('TZ_domestic_Ngram_stopword_lematize.csv', 'df_tz.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_tz.pkl', 'df_counts_tz1.pkl', 'df_counts_tz2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'salaam', 'dar', 'e', 'tanzania', 'zanzibar', 'tanzanians', 'prof'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_tz1.pkl', \n",
    "               'df_counts_tz2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_tz1.pkl',\n",
    "               'df_counts_tz2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('government', 10537))\n",
      "(1, ('country', 9670))\n",
      "(2, ('people', 6117))\n",
      "(3, ('use', 5641))\n",
      "(4, ('development', 4884))\n",
      "(5, ('project', 4395))\n",
      "(6, ('include', 4188))\n",
      "(7, ('need', 4073))\n",
      "(8, ('work', 4071))\n",
      "(9, ('new', 4009))\n",
      "(10, ('service', 3995))\n",
      "(11, ('minister', 3945))\n",
      "(12, ('president', 3807))\n",
      "(13, ('time', 3698))\n",
      "(14, ('business', 3576))\n",
      "(15, ('sector', 3523))\n",
      "(16, ('school', 3431))\n",
      "(17, ('national', 3425))\n",
      "(18, ('company', 3355))\n",
      "(19, ('public', 3353))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('rural', 884))\n",
      "(281, ('port', 883))\n",
      "(282, ('plant', 883))\n",
      "(283, ('key', 877))\n",
      "(284, ('income', 876))\n",
      "(285, ('bring', 869))\n",
      "(286, ('global', 869))\n",
      "(287, ('free', 863))\n",
      "(288, ('direct', 863))\n",
      "(289, ('measure', 858))\n",
      "(290, ('farm', 857))\n",
      "(291, ('join', 851))\n",
      "(292, ('control', 851))\n",
      "(293, ('body', 850))\n",
      "(294, ('view', 849))\n",
      "(295, ('despite', 846))\n",
      "(296, ('tourism', 845))\n",
      "(297, ('complete', 844))\n",
      "(298, ('recently', 844))\n",
      "(299, ('network', 842))\n"
     ]
    }
   ],
   "source": [
    "df_counts_tz_nostopwords = pd.read_pickle('df_counts_tz2.pkl')\n",
    "\n",
    "df_counts_tz_nostopwords = dict(zip(df_counts_tz_nostopwords[0], df_counts_tz_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_tz_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## United States US\n",
    "read_sort_save('US_domestic_Ngram_stopword_lematize.csv', 'df_us.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts('df_us.pkl', 'df_counts_us1.pkl', 'df_counts_us2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = [\n",
    "                'trump', 'facebook', 'obama'\n",
    "              ]\n",
    "\n",
    "[remove_list.append(i) for i in remove_items if i not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_clean('df_counts_us1.pkl', \n",
    "               'df_counts_us2.pkl',\n",
    "               remove_list,\n",
    "               stopwords,\n",
    "               'df_counts_us1.pkl',\n",
    "               'df_counts_us2.pkl'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('people', 92775))\n",
      "(1, ('time', 91124))\n",
      "(2, ('like', 90828))\n",
      "(3, ('new', 74341))\n",
      "(4, ('use', 70516))\n",
      "(5, ('work', 67046))\n",
      "(6, ('come', 59949))\n",
      "(7, ('know', 58216))\n",
      "(8, ('way', 51141))\n",
      "(9, ('want', 47687))\n",
      "(10, ('look', 45199))\n",
      "(11, ('think', 45198))\n",
      "(12, ('world', 43895))\n",
      "(13, ('include', 43605))\n",
      "(14, ('company', 43497))\n",
      "(15, ('need', 43304))\n",
      "(16, ('state', 42442))\n",
      "(17, ('life', 42273))\n",
      "(18, ('right', 41205))\n",
      "(19, ('high', 41018))\n",
      "\n",
      "\t ... \n",
      "\n",
      "(280, ('clear', 10851))\n",
      "(281, ('moment', 10830))\n",
      "(282, ('available', 10789))\n",
      "(283, ('leave', 10771))\n",
      "(284, ('fight', 10768))\n",
      "(285, ('air', 10763))\n",
      "(286, ('phone', 10761))\n",
      "(287, ('mother', 10760))\n",
      "(288, ('bring', 10682))\n",
      "(289, ('model', 10595))\n",
      "(290, ('hope', 10591))\n",
      "(291, ('content', 10589))\n",
      "(292, ('strong', 10556))\n",
      "(293, ('stand', 10543))\n",
      "(294, ('light', 10537))\n",
      "(295, ('account', 10531))\n",
      "(296, ('college', 10506))\n",
      "(297, ('outside', 10499))\n",
      "(298, ('access', 10493))\n",
      "(299, ('network', 10449))\n"
     ]
    }
   ],
   "source": [
    "df_counts_us_nostopwords = pd.read_pickle('df_counts_us2.pkl')\n",
    "\n",
    "df_counts_us_nostopwords = dict(zip(df_counts_us_nostopwords[0], df_counts_us_nostopwords[1]))\n",
    "\n",
    "wordcounts_print_sample(df_counts_us_nostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## South Africa ZA\n",
    "## Omitted due to insufficient articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_au_nostopwords = pd.read_pickle('df_counts_au2.pkl')\n",
    "dict_au = dict(zip(df_counts_au_nostopwords[0], df_counts_au_nostopwords[1]))\n",
    "dict_au = dict(Counter(dict_au).most_common(300))\n",
    "\n",
    "df_counts_bd_nostopwords = pd.read_pickle('df_counts_bd2.pkl')\n",
    "dict_bd = dict(zip(df_counts_bd_nostopwords[0], df_counts_bd_nostopwords[1]))\n",
    "dict_bd = dict(Counter(dict_bd).most_common(300))\n",
    "\n",
    "df_counts_ca_nostopwords = pd.read_pickle('df_counts_ca2.pkl')\n",
    "dict_ca = dict(zip(df_counts_ca_nostopwords[0], df_counts_ca_nostopwords[1]))\n",
    "dict_ca = dict(Counter(dict_ca).most_common(300))\n",
    "\n",
    "df_counts_gb_nostopwords = pd.read_pickle('df_counts_gb2.pkl')\n",
    "dict_gb = dict(zip(df_counts_gb_nostopwords[0], df_counts_gb_nostopwords[1]))\n",
    "dict_gb = dict(Counter(dict_gb).most_common(300))\n",
    "\n",
    "df_counts_gh_nostopwords = pd.read_pickle('df_counts_gh2.pkl')\n",
    "dict_gh = dict(zip(df_counts_gh_nostopwords[0], df_counts_gh_nostopwords[1]))\n",
    "dict_gh = dict(Counter(dict_gh).most_common(300))\n",
    "\n",
    "df_counts_hk_nostopwords = pd.read_pickle('df_counts_hk2.pkl')\n",
    "dict_hk = dict(zip(df_counts_hk_nostopwords[0], df_counts_hk_nostopwords[1]))\n",
    "dict_hk = dict(Counter(dict_hk).most_common(300))\n",
    "\n",
    "df_counts_ie_nostopwords = pd.read_pickle('df_counts_ie2.pkl')\n",
    "dict_ie = dict(zip(df_counts_ie_nostopwords[0], df_counts_ie_nostopwords[1]))\n",
    "dict_ie = dict(Counter(dict_ie).most_common(300))\n",
    "\n",
    "df_counts_in_nostopwords = pd.read_pickle('df_counts_in2.pkl')\n",
    "dict_in = dict(zip(df_counts_in_nostopwords[0], df_counts_in_nostopwords[1]))\n",
    "dict_in = dict(Counter(dict_in).most_common(300))\n",
    "\n",
    "df_counts_jm_nostopwords = pd.read_pickle('df_counts_jm2.pkl')\n",
    "dict_jm = dict(zip(df_counts_jm_nostopwords[0], df_counts_jm_nostopwords[1]))\n",
    "dict_jm = dict(Counter(dict_jm).most_common(300))\n",
    "\n",
    "df_counts_ke_nostopwords = pd.read_pickle('df_counts_ke2.pkl')\n",
    "dict_ke = dict(zip(df_counts_ke_nostopwords[0], df_counts_ke_nostopwords[1]))\n",
    "dict_ke = dict(Counter(dict_ke).most_common(300))\n",
    "\n",
    "df_counts_lk_nostopwords = pd.read_pickle('df_counts_lk2.pkl')\n",
    "dict_lk = dict(zip(df_counts_lk_nostopwords[0], df_counts_lk_nostopwords[1]))\n",
    "dict_lk = dict(Counter(dict_lk).most_common(300))\n",
    "\n",
    "df_counts_my_nostopwords = pd.read_pickle('df_counts_my2.pkl')\n",
    "dict_my = dict(zip(df_counts_my_nostopwords[0], df_counts_my_nostopwords[1]))\n",
    "dict_my = dict(Counter(dict_my).most_common(300))\n",
    "\n",
    "df_counts_ng_nostopwords = pd.read_pickle('df_counts_ng2.pkl')\n",
    "dict_ng = dict(zip(df_counts_ng_nostopwords[0], df_counts_ng_nostopwords[1]))\n",
    "dict_ng = dict(Counter(dict_ng).most_common(300))\n",
    "\n",
    "df_counts_nz_nostopwords = pd.read_pickle('df_counts_nz2.pkl')\n",
    "dict_nz = dict(zip(df_counts_nz_nostopwords[0], df_counts_nz_nostopwords[1]))\n",
    "dict_nz = dict(Counter(dict_nz).most_common(300))\n",
    "\n",
    "df_counts_ph_nostopwords = pd.read_pickle('df_counts_ph2.pkl')\n",
    "dict_ph = df_counts_ph_nostopwords\n",
    "dict_ph = dict(Counter(dict_ph).most_common(300))\n",
    "\n",
    "df_counts_sg_nostopwords = pd.read_pickle('df_counts_sg2.pkl')\n",
    "dict_sg = dict(zip(df_counts_sg_nostopwords[0], df_counts_sg_nostopwords[1]))\n",
    "dict_sg = dict(Counter(dict_sg).most_common(300))\n",
    "\n",
    "df_counts_tz_nostopwords = pd.read_pickle('df_counts_tz2.pkl')\n",
    "dict_tz = dict(zip(df_counts_tz_nostopwords[0], df_counts_tz_nostopwords[1]))\n",
    "dict_tz = dict(Counter(dict_tz).most_common(300))\n",
    "\n",
    "df_counts_us_nostopwords = pd.read_pickle('df_counts_us2.pkl')\n",
    "dict_us = dict(zip(df_counts_us_nostopwords[0], df_counts_us_nostopwords[1]))\n",
    "dict_us = dict(Counter(dict_us).most_common(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dict_au,dict_bd,dict_ca,dict_gb,dict_gh,dict_hk,dict_ie,dict_in,dict_jm,dict_ke,dict_lk,dict_my,dict_ng,dict_nz,dict_ph,dict_sg,dict_tz,dict_us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>people</th>\n",
       "      <th>like</th>\n",
       "      <th>new</th>\n",
       "      <th>work</th>\n",
       "      <th>use</th>\n",
       "      <th>come</th>\n",
       "      <th>need</th>\n",
       "      <th>government</th>\n",
       "      <th>look</th>\n",
       "      <th>...</th>\n",
       "      <th>natural</th>\n",
       "      <th>meet</th>\n",
       "      <th>implementation</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>union</th>\n",
       "      <th>rural</th>\n",
       "      <th>port</th>\n",
       "      <th>white</th>\n",
       "      <th>character</th>\n",
       "      <th>effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australia AU</th>\n",
       "      <td>99918</td>\n",
       "      <td>83357</td>\n",
       "      <td>73990</td>\n",
       "      <td>69815</td>\n",
       "      <td>65096</td>\n",
       "      <td>63113</td>\n",
       "      <td>57492</td>\n",
       "      <td>51243.0</td>\n",
       "      <td>51129</td>\n",
       "      <td>49322.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bangladesh BD</th>\n",
       "      <td>12261</td>\n",
       "      <td>19966</td>\n",
       "      <td>7089</td>\n",
       "      <td>8614</td>\n",
       "      <td>11527</td>\n",
       "      <td>8819</td>\n",
       "      <td>9810</td>\n",
       "      <td>7700.0</td>\n",
       "      <td>20966</td>\n",
       "      <td>3217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada CA</th>\n",
       "      <td>101831</td>\n",
       "      <td>96695</td>\n",
       "      <td>78735</td>\n",
       "      <td>80842</td>\n",
       "      <td>78459</td>\n",
       "      <td>81684</td>\n",
       "      <td>69959</td>\n",
       "      <td>51774.0</td>\n",
       "      <td>49154</td>\n",
       "      <td>55700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom GB</th>\n",
       "      <td>48544</td>\n",
       "      <td>48863</td>\n",
       "      <td>39139</td>\n",
       "      <td>37046</td>\n",
       "      <td>38795</td>\n",
       "      <td>32345</td>\n",
       "      <td>33054</td>\n",
       "      <td>23568.0</td>\n",
       "      <td>18847</td>\n",
       "      <td>24954.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ghana GH</th>\n",
       "      <td>16691</td>\n",
       "      <td>22788</td>\n",
       "      <td>9709</td>\n",
       "      <td>14518</td>\n",
       "      <td>14492</td>\n",
       "      <td>13881</td>\n",
       "      <td>14847</td>\n",
       "      <td>13132.0</td>\n",
       "      <td>26010</td>\n",
       "      <td>5758.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hong Kong HK</th>\n",
       "      <td>3051</td>\n",
       "      <td>3349</td>\n",
       "      <td>1585</td>\n",
       "      <td>2787</td>\n",
       "      <td>2053</td>\n",
       "      <td>1928</td>\n",
       "      <td>1723</td>\n",
       "      <td>1424.0</td>\n",
       "      <td>3738</td>\n",
       "      <td>987.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland IE</th>\n",
       "      <td>77019</td>\n",
       "      <td>61005</td>\n",
       "      <td>42155</td>\n",
       "      <td>47556</td>\n",
       "      <td>48931</td>\n",
       "      <td>72634</td>\n",
       "      <td>50201</td>\n",
       "      <td>28496.0</td>\n",
       "      <td>18515</td>\n",
       "      <td>31373.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India IN</th>\n",
       "      <td>69275</td>\n",
       "      <td>59907</td>\n",
       "      <td>56214</td>\n",
       "      <td>61042</td>\n",
       "      <td>49861</td>\n",
       "      <td>43461</td>\n",
       "      <td>57492</td>\n",
       "      <td>37376.0</td>\n",
       "      <td>73660</td>\n",
       "      <td>30884.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jamaica JM</th>\n",
       "      <td>29117</td>\n",
       "      <td>27789</td>\n",
       "      <td>16414</td>\n",
       "      <td>23885</td>\n",
       "      <td>27897</td>\n",
       "      <td>21393</td>\n",
       "      <td>22996</td>\n",
       "      <td>21383.0</td>\n",
       "      <td>26488</td>\n",
       "      <td>11887.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenya KE</th>\n",
       "      <td>21596</td>\n",
       "      <td>22643</td>\n",
       "      <td>13510</td>\n",
       "      <td>16562</td>\n",
       "      <td>16792</td>\n",
       "      <td>16317</td>\n",
       "      <td>17559</td>\n",
       "      <td>13321.0</td>\n",
       "      <td>28224</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sri Lanka LK</th>\n",
       "      <td>10794</td>\n",
       "      <td>13226</td>\n",
       "      <td>5550</td>\n",
       "      <td>9108</td>\n",
       "      <td>7629</td>\n",
       "      <td>7914</td>\n",
       "      <td>7976</td>\n",
       "      <td>7023.0</td>\n",
       "      <td>17051</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia MY</th>\n",
       "      <td>24115</td>\n",
       "      <td>25653</td>\n",
       "      <td>17897</td>\n",
       "      <td>24631</td>\n",
       "      <td>17900</td>\n",
       "      <td>23905</td>\n",
       "      <td>17464</td>\n",
       "      <td>17413.0</td>\n",
       "      <td>34692</td>\n",
       "      <td>12003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nigeria NG</th>\n",
       "      <td>53674</td>\n",
       "      <td>87846</td>\n",
       "      <td>40458</td>\n",
       "      <td>39288</td>\n",
       "      <td>36905</td>\n",
       "      <td>36815</td>\n",
       "      <td>50587</td>\n",
       "      <td>39026.0</td>\n",
       "      <td>101682</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand NZ</th>\n",
       "      <td>66007</td>\n",
       "      <td>75023</td>\n",
       "      <td>44970</td>\n",
       "      <td>56268</td>\n",
       "      <td>62730</td>\n",
       "      <td>40259</td>\n",
       "      <td>45495</td>\n",
       "      <td>43598.0</td>\n",
       "      <td>36245</td>\n",
       "      <td>33152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines PH</th>\n",
       "      <td>50058</td>\n",
       "      <td>43767</td>\n",
       "      <td>36488</td>\n",
       "      <td>39723</td>\n",
       "      <td>18533</td>\n",
       "      <td>38808</td>\n",
       "      <td>9948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore SG</th>\n",
       "      <td>16261</td>\n",
       "      <td>13069</td>\n",
       "      <td>12335</td>\n",
       "      <td>21763</td>\n",
       "      <td>15171</td>\n",
       "      <td>14346</td>\n",
       "      <td>11514</td>\n",
       "      <td>10567.0</td>\n",
       "      <td>8432</td>\n",
       "      <td>9331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tanzania TZ</th>\n",
       "      <td>3698</td>\n",
       "      <td>6117</td>\n",
       "      <td>1735</td>\n",
       "      <td>4009</td>\n",
       "      <td>4071</td>\n",
       "      <td>5641</td>\n",
       "      <td>3082</td>\n",
       "      <td>4073.0</td>\n",
       "      <td>10537</td>\n",
       "      <td>966.0</td>\n",
       "      <td>...</td>\n",
       "      <td>934.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>916.0</td>\n",
       "      <td>889.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States US</th>\n",
       "      <td>91124</td>\n",
       "      <td>92775</td>\n",
       "      <td>90828</td>\n",
       "      <td>74341</td>\n",
       "      <td>67046</td>\n",
       "      <td>70516</td>\n",
       "      <td>59949</td>\n",
       "      <td>43304.0</td>\n",
       "      <td>28283</td>\n",
       "      <td>45199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13136.0</td>\n",
       "      <td>11767.0</td>\n",
       "      <td>10871.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  people   like    new   work    use   come     need  \\\n",
       "Australia AU        99918   83357  73990  69815  65096  63113  57492  51243.0   \n",
       "Bangladesh BD       12261   19966   7089   8614  11527   8819   9810   7700.0   \n",
       "Canada CA          101831   96695  78735  80842  78459  81684  69959  51774.0   \n",
       "United Kingdom GB   48544   48863  39139  37046  38795  32345  33054  23568.0   \n",
       "Ghana GH            16691   22788   9709  14518  14492  13881  14847  13132.0   \n",
       "Hong Kong HK         3051    3349   1585   2787   2053   1928   1723   1424.0   \n",
       "Ireland IE          77019   61005  42155  47556  48931  72634  50201  28496.0   \n",
       "India IN            69275   59907  56214  61042  49861  43461  57492  37376.0   \n",
       "Jamaica JM          29117   27789  16414  23885  27897  21393  22996  21383.0   \n",
       "Kenya KE            21596   22643  13510  16562  16792  16317  17559  13321.0   \n",
       "Sri Lanka LK        10794   13226   5550   9108   7629   7914   7976   7023.0   \n",
       "Malaysia MY         24115   25653  17897  24631  17900  23905  17464  17413.0   \n",
       "Nigeria NG          53674   87846  40458  39288  36905  36815  50587  39026.0   \n",
       "New Zealand NZ      66007   75023  44970  56268  62730  40259  45495  43598.0   \n",
       "Philippines PH      50058   43767  36488  39723  18533  38808   9948      NaN   \n",
       "Singapore SG        16261   13069  12335  21763  15171  14346  11514  10567.0   \n",
       "Tanzania TZ          3698    6117   1735   4009   4071   5641   3082   4073.0   \n",
       "United States US    91124   92775  90828  74341  67046  70516  59949  43304.0   \n",
       "\n",
       "                   government     look  ...  natural   meet  implementation  \\\n",
       "Australia AU            51129  49322.0  ...      NaN    NaN             NaN   \n",
       "Bangladesh BD           20966   3217.0  ...      NaN    NaN             NaN   \n",
       "Canada CA               49154  55700.0  ...      NaN    NaN             NaN   \n",
       "United Kingdom GB       18847  24954.0  ...      NaN    NaN             NaN   \n",
       "Ghana GH                26010   5758.0  ...      NaN    NaN             NaN   \n",
       "Hong Kong HK             3738    987.0  ...      NaN    NaN             NaN   \n",
       "Ireland IE              18515  31373.0  ...      NaN    NaN             NaN   \n",
       "India IN                73660  30884.0  ...      NaN    NaN             NaN   \n",
       "Jamaica JM              26488  11887.0  ...      NaN    NaN             NaN   \n",
       "Kenya KE                28224   7517.0  ...      NaN    NaN             NaN   \n",
       "Sri Lanka LK            17051   3298.0  ...      NaN    NaN             NaN   \n",
       "Malaysia MY             34692  12003.0  ...      NaN    NaN             NaN   \n",
       "Nigeria NG             101682  19914.0  ...      NaN    NaN             NaN   \n",
       "New Zealand NZ          36245  33152.0  ...      NaN    NaN             NaN   \n",
       "Philippines PH          52619      NaN  ...      NaN    NaN             NaN   \n",
       "Singapore SG             8432   9331.0  ...      NaN    NaN             NaN   \n",
       "Tanzania TZ             10537    966.0  ...    934.0  917.0           916.0   \n",
       "United States US        28283  45199.0  ...      NaN    NaN             NaN   \n",
       "\n",
       "                   infrastructure  union  rural   port    white  character  \\\n",
       "Australia AU                  NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Bangladesh BD                 NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Canada CA                     NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "United Kingdom GB             NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Ghana GH                      NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Hong Kong HK                  NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Ireland IE                    NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "India IN                      NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Jamaica JM                    NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Kenya KE                      NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Sri Lanka LK                  NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Malaysia MY                   NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Nigeria NG                    NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "New Zealand NZ                NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Philippines PH                NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Singapore SG                  NaN    NaN    NaN    NaN      NaN        NaN   \n",
       "Tanzania TZ                 889.0  886.0  884.0  883.0      NaN        NaN   \n",
       "United States US              NaN    NaN    NaN    NaN  13136.0    11767.0   \n",
       "\n",
       "                    effect  \n",
       "Australia AU           NaN  \n",
       "Bangladesh BD          NaN  \n",
       "Canada CA              NaN  \n",
       "United Kingdom GB      NaN  \n",
       "Ghana GH               NaN  \n",
       "Hong Kong HK           NaN  \n",
       "Ireland IE             NaN  \n",
       "India IN               NaN  \n",
       "Jamaica JM             NaN  \n",
       "Kenya KE               NaN  \n",
       "Sri Lanka LK           NaN  \n",
       "Malaysia MY            NaN  \n",
       "Nigeria NG             NaN  \n",
       "New Zealand NZ         NaN  \n",
       "Philippines PH         NaN  \n",
       "Singapore SG           NaN  \n",
       "Tanzania TZ            NaN  \n",
       "United States US   10871.0  \n",
       "\n",
       "[18 rows x 777 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_countries_no_stopwords = pd.DataFrame.from_dict(data)\n",
    "all_countries_no_stopwords.index = ['Australia AU', 'Bangladesh BD', 'Canada CA', 'United Kingdom GB',\n",
    "                                    'Ghana GH','Hong Kong HK','Ireland IE','India IN',\n",
    "                                    'Jamaica JM','Kenya KE','Sri Lanka LK','Malaysia MY',\n",
    "                                    'Nigeria NG','New Zealand NZ','Philippines PH','Singapore SG',\n",
    "                                    'Tanzania TZ','United States US']\n",
    "\n",
    "all_countries_no_stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
